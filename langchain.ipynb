{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt_files(folder_path: str) -> List[str]:\n",
    "    loader = DirectoryLoader(path=folder_path, show_progress=True, use_multithreading=True)\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(docs: List[str], chunk_size: int = 1000, chunk_overlap: int = 200) -> List[str]:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_model() -> HuggingFaceEmbeddings:\n",
    "    model_name = \"all-mpnet-base-v2\"\n",
    "    model_kwargs = {\n",
    "        'device': 'cuda',\n",
    "        'trust_remote_code': True,\n",
    "        'token': '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'\n",
    "    }\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "        show_progress=True\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_embeddings(texts: List[str], embeddings: HuggingFaceEmbeddings) -> FAISS:\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(vectorstore: FAISS, path: str) -> None:\n",
    "    vectorstore.save_local(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path: str, embeddings: HuggingFaceEmbeddings) -> FAISS:\n",
    "    return FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]/media/basal-desktop/E/Dynamic_Rag/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 6/6 [00:08<00:00,  1.39s/it]\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"./Data/\"\n",
    "docs = load_txt_files(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = split_text(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/basal-desktop/E/Dynamic_Rag/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Batches: 100%|██████████| 14/14 [00:02<00:00,  6.84it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = create_embeddings_model()\n",
    "vectorstore = apply_embeddings(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings(vectorstore, \"Langchain/Cars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/basal-desktop/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens= 2024,\n",
    "    temperature= 0.1,\n",
    "    repetition_penalty= 1.03,\n",
    "    huggingfacehub_api_token=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "As a medical assistant, your role is to provide support and assistance to the user in various medical fields.\n",
    "You are expected to have a strong understanding of medical terminology, procedures, and patient care. When clients ask questions, \n",
    "it is important to respond accurately and within the scope of your expertise. If you don't know the answer to a question, \n",
    "it is important to be honest and communicate that you are unsure rather than providing incorrect information. \n",
    "Your knowledge and skills in the medical field are essential for delivering high-quality care to patients.\n",
    "it is important to the answer in a detailed, consice and structured manner and please provide answer only in English language.\n",
    "Don't start your answer with 'based on the context' or 'As a medical assistent' something like that and you don't need to mention about sources.\n",
    "                                          \n",
    "                                          \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide a detailed, concise and structured answer. \n",
      "\n",
      "Answer: \n",
      "Senescent cells, which are cells that have entered a state of permanent cell cycle arrest, play a crucial role in chronic inflammation in aging. These cells produce pro-inflammatory cytokines and chemokines, which attract immune cells to the site of senescence, leading to chronic inflammation. This process is known as the \"senescence-associated secretory phenotype\" (SASP). \n",
      "\n",
      "The SASP promotes the recruitment of immune cells, such as macrophages and T-cells, which further exacerbate inflammation. This chronic inflammation can lead to tissue damage, organ dysfunction, and age-related diseases, such as atherosclerosis, osteoarthritis, and cancer. \n",
      "\n",
      "Therapies targeting senescent cells aim to eliminate or reduce their numbers, thereby reducing chronic inflammation and its associated consequences. These therapies include senolytic drugs, which selectively kill senescent cells, and senostatic drugs, which inhibit the SASP. \n",
      "\n",
      "Benefits of these therapies include reduced chronic inflammation, improved tissue function, and potentially delayed or prevented age-related diseases. However, there are also risks associated with these therapies, such as off-target effects on healthy cells and potential immune suppression. \n",
      "\n",
      "Therefore, it is essential to carefully evaluate the benefits and risks of senolytic and senostatic therapies and to conduct rigorous clinical trials to ensure their safety and efficacy in humans. \n",
      "\n",
      "In conclusion, senescent cells contribute to chronic inflammation in aging through the SASP, and therapies targeting these cells may offer benefits in reducing chronic inflammation and age-related diseases. However, careful consideration of the risks and benefits is necessary to ensure the safe and effective use of these therapies.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"How do senescent cells contribute to chronic inflammation in aging, and what are the benefits and risks of therapies targeting these cells?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
